{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qoDhL2LGO0f"
   },
   "source": [
    "# **Семинар 3 — Создание gRPC-сервиса для ML-модели (protobuf + Python)**\n",
    "\n",
    "## **Цель занятия**\n",
    "\n",
    "Освоить базовые навыки проектирования и реализации gRPC-сервиса для ML-модели:  \n",
    "описать контракт в **Protocol Buffers**, сгенерировать сервер и клиента на Python,  \n",
    "реализовать эндпоинты `/health` и `/predict`, запустить локальные тесты производительности  \n",
    "и подготовить структуру проекта к дальнейшей контейнеризации.\n",
    "\n",
    "---\n",
    "\n",
    "## **План занятия**\n",
    "\n",
    "1. Установка инструментов и подготовка окружения  \n",
    "2. Проектирование контракта API в `.proto`  \n",
    "3. Генерация Python-кода из protobuf  \n",
    "4. Реализация сервера gRPC с загрузкой модели  \n",
    "5. Реализация клиента gRPC и базовых тестов  \n",
    "6. Валидация входных данных, обработка ошибок, таймауты  \n",
    "7. Мини-бенчмарк: сравнение с REST (по желанию)  \n",
    "8. Итог: чек-лист готовности к контейнеризации\n",
    "\n",
    "---\n",
    "\n",
    "## **Предварительные требования**\n",
    "\n",
    "- Python 3.10+  \n",
    "- Виртуальное окружение (venv или conda)  \n",
    "- Пакеты: `grpcio`, `grpcio-tools`, `pandas`, `scikit-learn`, `joblib`, `uvloop` (опционально под Linux)  \n",
    "- Заготовленная модель `model.pkl` (например, LogisticRegression, обученная на Iris или Wine)\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Структура проекта (Обновленная)**\n",
    "\n",
    "Root/\n",
    "\n",
    "protos/\n",
    "\n",
    "model.proto\n",
    "\n",
    "server/\n",
    "\n",
    "server.py\n",
    "\n",
    "inference.py\n",
    "\n",
    "validation.py\n",
    "\n",
    "client/\n",
    "\n",
    "client.py\n",
    "\n",
    "models/\n",
    "\n",
    "model.pkl\n",
    "\n",
    "requirements.txt\n",
    "\n",
    "Makefile\n",
    "\n",
    "README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N0ezipPOGPIt"
   },
   "outputs": [],
   "source": [
    "# Установка необходимых библиотек\n",
    "!pip install grpcio==1.66.1 grpcio-tools==1.66.1 pandas scikit-learn joblib uvloop -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDGTpdavGftF"
   },
   "source": [
    "## **2. Создание структуры проекта**\n",
    "\n",
    "Создадим базовую структуру папок для gRPC-сервиса в текущей директории.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5gZ-CQkUGnZb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Структура каталогов создана.\n",
      "zsh:1: command not found: tree\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Обновленные пути (без префикса ml_grpc_service)\n",
    "folders = [\n",
    "    \"protos\",\n",
    "    \"server\",\n",
    "    \"client\",\n",
    "    \"models\"\n",
    "]\n",
    "for f in folders:\n",
    "    os.makedirs(f, exist_ok=True)\n",
    "\n",
    "print(\"✅ Структура каталогов создана.\")\n",
    "!tree .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6NzgbKdGpSJ"
   },
   "source": [
    "## **3. Проектирование контракта API — `model.proto`**\n",
    "\n",
    "Файл `.proto` описывает, какие методы и типы данных доступны клиенту и серверу.  \n",
    "Ниже — минимальный, но расширяемый контракт с двумя RPC-методами: **Health** и **Predict**.  \n",
    "Добавлено поле `model_version` и возможность передавать произвольные числовые признаки в виде повторяющегося поля.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mpynuvQ7Grhz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Файл model.proto создан:\n",
      "syntax = \"proto3\";\n",
      "package mlservice.v1;\n",
      "\n",
      "service PredictionService {\n",
      "  rpc Health(HealthRequest) returns (HealthResponse);\n",
      "  rpc Predict(PredictRequest) returns (PredictResponse);\n",
      "}\n",
      "\n",
      "message HealthRequest {}\n",
      "\n",
      "message HealthResponse {\n",
      "  string status = 1;         // \"ok\"\n",
      "  string model_version = 2;  // e.g. \"v1.0.3\"\n",
      "}\n",
      "\n",
      "message Feature {\n",
      "  string name = 1;\n",
      "  double value = 2;\n",
      "}\n",
      "\n",
      "message PredictRequest {\n",
      "  repeated Feature features = 1;  // [{name:\"sepal_length\", value:5.1}, ...]\n",
      "}\n",
      "\n",
      "message PredictResponse {\n",
      "  string prediction = 1;          // \"setosa\"\n",
      "  double confidence = 2;          // 0.93 (опционально)\n",
      "  string model_version = 3;\n",
      "}"
     ]
    }
   ],
   "source": [
    "proto_code = \"\"\"\n",
    "syntax = \"proto3\";\n",
    "package mlservice.v1;\n",
    "\n",
    "service PredictionService {\n",
    "  rpc Health(HealthRequest) returns (HealthResponse);\n",
    "  rpc Predict(PredictRequest) returns (PredictResponse);\n",
    "}\n",
    "\n",
    "message HealthRequest {}\n",
    "\n",
    "message HealthResponse {\n",
    "  string status = 1;         // \"ok\"\n",
    "  string model_version = 2;  // e.g. \"v1.0.3\"\n",
    "}\n",
    "\n",
    "message Feature {\n",
    "  string name = 1;\n",
    "  double value = 2;\n",
    "}\n",
    "\n",
    "message PredictRequest {\n",
    "  repeated Feature features = 1;  // [{name:\"sepal_length\", value:5.1}, ...]\n",
    "}\n",
    "\n",
    "message PredictResponse {\n",
    "  string prediction = 1;          // \"setosa\"\n",
    "  double confidence = 2;          // 0.93 (опционально)\n",
    "  string model_version = 3;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open(\"protos/model.proto\", \"w\") as f:\n",
    "    f.write(proto_code.strip())\n",
    "\n",
    "print(\"✅ Файл model.proto создан:\")\n",
    "!cat protos/model.proto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cxQ4EVvGtrD"
   },
   "source": [
    "## **4. Генерация Python-кода из protobuf**\n",
    "\n",
    "Теперь скомпилируем `.proto` в Python-код.  \n",
    "Команда `protoc` из пакета `grpcio-tools` создаёт два файла:\n",
    "- `model_pb2.py` — описания сообщений;\n",
    "- `model_pb2_grpc.py` — интерфейс сервиса.\n",
    "\n",
    "Мы генерируем файлы прямо в корень проекта (root), чтобы упростить импорты.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bOWVH1AiGvKW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Код сгенерирован:\n",
      "\u001b[34mmodels\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# Генерация файлов в текущую директорию (.)\n",
    "!python -m grpc_tools.protoc -I. \\\n",
    "  --python_out=. \\\n",
    "  --grpc_python_out=. \\\n",
    "  protos/model.proto\n",
    "\n",
    "print(\"✅ Код сгенерирован:\")\n",
    "!ls | grep model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVi-X1guGxuS"
   },
   "source": [
    "## **5. Логика инференса и валидации данных**\n",
    "\n",
    "### **5.1 inference.py**\n",
    "\n",
    "Файл `server/inference.py` отвечает за загрузку модели и выполнение предсказаний.  \n",
    "Для примера используется модель `model.pkl`, обученная заранее (например, LogisticRegression на Iris).  \n",
    "Метод `predict()` возвращает предсказанный класс и confidence (если доступен `predict_proba`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rMESSI3kG22H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ inference.py создан.\n",
      "import joblib\n",
      "import pandas as pd\n",
      "from pathlib import Path\n",
      "\n",
      "class ModelRunner:\n",
      "    def __init__(self, model_path: str, version: str = \"v1.0.0\"):\n",
      "        self.model = joblib.load(model_path)\n",
      "        self.version = version\n",
      "\n",
      "    def predict(self, features: dict[str, float]) -> tuple[str, float]:\n",
      "        df = pd.DataFrame([features])\n",
      "        y = self.model.predict(df)[0]\n",
      "        try:\n",
      "            proba = float(max(self.model.predict_proba(df)[0]))\n",
      "        except Exception:\n",
      "            proba = 1.0\n",
      "        return str(y), proba"
     ]
    }
   ],
   "source": [
    "inference_code = \"\"\"\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "class ModelRunner:\n",
    "    def __init__(self, model_path: str, version: str = \"v1.0.0\"):\n",
    "        self.model = joblib.load(model_path)\n",
    "        self.version = version\n",
    "\n",
    "    def predict(self, features: dict[str, float]) -> tuple[str, float]:\n",
    "        df = pd.DataFrame([features])\n",
    "        y = self.model.predict(df)[0]\n",
    "        try:\n",
    "            proba = float(max(self.model.predict_proba(df)[0]))\n",
    "        except Exception:\n",
    "            proba = 1.0\n",
    "        return str(y), proba\n",
    "\"\"\"\n",
    "\n",
    "with open(\"server/inference.py\", \"w\") as f:\n",
    "    f.write(inference_code.strip())\n",
    "\n",
    "print(\"✅ inference.py создан.\")\n",
    "!head -n 20 server/inference.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuxMqDzGG4Uf"
   },
   "source": [
    "### **5.2 validation.py**\n",
    "\n",
    "Файл `server/validation.py` отвечает за проверку входных данных:\n",
    "- отсутствие дубликатов признаков,\n",
    "- отсутствие пустых имён,\n",
    "- наличие хотя бы одного признака.\n",
    "\n",
    "При ошибке выбрасывается исключение `ValidationError`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "53J3Cpv5G6Kn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ validation.py создан.\n",
      "from typing import Iterable\n",
      "import model_pb2  # Import from root\n",
      "\n",
      "class ValidationError(Exception):\n",
      "    pass\n",
      "\n",
      "def features_to_dict(features: Iterable[model_pb2.Feature]) -> dict[str, float]:\n",
      "    data = {}\n",
      "    for f in features:\n",
      "        if f.name in data:\n",
      "            raise ValidationError(f\"Duplicate feature: {f.name}\")\n",
      "        if not f.name:\n",
      "            raise ValidationError(\"Empty feature name\")\n",
      "        data[f.name] = float(f.value)\n",
      "    if not data:\n",
      "        raise ValidationError(\"No features provided\")\n",
      "    return data"
     ]
    }
   ],
   "source": [
    "validation_code = \"\"\"\n",
    "from typing import Iterable\n",
    "import model_pb2  # Import from root\n",
    "\n",
    "class ValidationError(Exception):\n",
    "    pass\n",
    "\n",
    "def features_to_dict(features: Iterable[model_pb2.Feature]) -> dict[str, float]:\n",
    "    data = {}\n",
    "    for f in features:\n",
    "        if f.name in data:\n",
    "            raise ValidationError(f\"Duplicate feature: {f.name}\")\n",
    "        if not f.name:\n",
    "            raise ValidationError(\"Empty feature name\")\n",
    "        data[f.name] = float(f.value)\n",
    "    if not data:\n",
    "        raise ValidationError(\"No features provided\")\n",
    "    return data\n",
    "\"\"\"\n",
    "\n",
    "with open(\"server/validation.py\", \"w\") as f:\n",
    "    f.write(validation_code.strip())\n",
    "\n",
    "print(\"✅ validation.py создан.\")\n",
    "!head -n 20 server/validation.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44jJkQFRG7l1"
   },
   "source": [
    "## **6. Реализация сервера gRPC**\n",
    "\n",
    "Сервер принимает запросы `/Health` и `/Predict`, выполняет валидацию входных данных,  \n",
    "делает предсказание через `ModelRunner` и возвращает результат клиенту.  \n",
    "Ошибки передаются через `context.set_code` и `context.set_details`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HbDverEQG9EJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ server.py создан.\n",
      "import grpc, os\n",
      "from concurrent import futures\n",
      "import model_pb2, model_pb2_grpc  # Imports from root\n",
      "from server.inference import ModelRunner\n",
      "from server.validation import features_to_dict, ValidationError\n",
      "\n",
      "MODEL_PATH = os.getenv(\"MODEL_PATH\", \"models/model.pkl\")\n",
      "MODEL_VERSION = os.getenv(\"MODEL_VERSION\", \"v1.0.0\")\n",
      "MAX_WORKERS = int(os.getenv(\"MAX_WORKERS\", \"4\"))\n",
      "PORT = int(os.getenv(\"PORT\", \"50051\"))\n",
      "\n",
      "class PredictionService(model_pb2_grpc.PredictionServiceServicer):\n",
      "    def __init__(self):\n",
      "        self.runner = ModelRunner(MODEL_PATH, version=MODEL_VERSION)\n",
      "\n",
      "    def Health(self, request, context):\n",
      "        return model_pb2.HealthResponse(status=\"ok\", model_version=self.runner.version)\n",
      "\n",
      "    def Predict(self, request, context):\n",
      "        try:\n",
      "            feats = features_to_dict(request.features)\n",
      "            pred, conf = self.runner.predict(feats)\n",
      "            return model_pb2.PredictResponse(\n",
      "                prediction=pred, confidence=conf, model_version=self.runner.version\n",
      "            )\n"
     ]
    }
   ],
   "source": [
    "server_code = \"\"\"\n",
    "import grpc, os\n",
    "from concurrent import futures\n",
    "import model_pb2, model_pb2_grpc  # Imports from root\n",
    "from server.inference import ModelRunner\n",
    "from server.validation import features_to_dict, ValidationError\n",
    "\n",
    "MODEL_PATH = os.getenv(\"MODEL_PATH\", \"models/model.pkl\")\n",
    "MODEL_VERSION = os.getenv(\"MODEL_VERSION\", \"v1.0.0\")\n",
    "MAX_WORKERS = int(os.getenv(\"MAX_WORKERS\", \"4\"))\n",
    "PORT = int(os.getenv(\"PORT\", \"50051\"))\n",
    "\n",
    "class PredictionService(model_pb2_grpc.PredictionServiceServicer):\n",
    "    def __init__(self):\n",
    "        self.runner = ModelRunner(MODEL_PATH, version=MODEL_VERSION)\n",
    "\n",
    "    def Health(self, request, context):\n",
    "        return model_pb2.HealthResponse(status=\"ok\", model_version=self.runner.version)\n",
    "\n",
    "    def Predict(self, request, context):\n",
    "        try:\n",
    "            feats = features_to_dict(request.features)\n",
    "            pred, conf = self.runner.predict(feats)\n",
    "            return model_pb2.PredictResponse(\n",
    "                prediction=pred, confidence=conf, model_version=self.runner.version\n",
    "            )\n",
    "        except ValidationError as ve:\n",
    "            context.set_code(grpc.StatusCode.INVALID_ARGUMENT)\n",
    "            context.set_details(str(ve))\n",
    "            return model_pb2.PredictResponse()\n",
    "        except Exception as e:\n",
    "            context.set_code(grpc.StatusCode.INTERNAL)\n",
    "            context.set_details(f\"internal error: {e}\")\n",
    "            return model_pb2.PredictResponse()\n",
    "\n",
    "def serve():\n",
    "    options = [\n",
    "        (\"grpc.max_send_message_length\", 50 * 1024 * 1024),\n",
    "        (\"grpc.max_receive_message_length\", 50 * 1024 * 1024),\n",
    "    ]\n",
    "    server = grpc.server(futures.ThreadPoolExecutor(max_workers=MAX_WORKERS), options=options)\n",
    "    model_pb2_grpc.add_PredictionServiceServicer_to_server(PredictionService(), server)\n",
    "    server.add_insecure_port(f\"[::]:{PORT}\")\n",
    "    server.start()\n",
    "    print(f\"gRPC server started on :{PORT}, model={MODEL_PATH}, version={MODEL_VERSION}\")\n",
    "    server.wait_for_termination()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        import uvloop\n",
    "        uvloop.install()\n",
    "    except Exception:\n",
    "        pass\n",
    "    serve()\n",
    "\"\"\"\n",
    "\n",
    "with open(\"server/server.py\", \"w\") as f:\n",
    "    f.write(server_code.strip())\n",
    "\n",
    "print(\"✅ server.py создан.\")\n",
    "!head -n 25 server/server.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wyOAgkTG-17"
   },
   "source": [
    "## **7. Клиент gRPC и ручные прогоны**\n",
    "\n",
    "Реализуем клиента, который умеет:\n",
    "- создавать `stub` для подключения к серверу;\n",
    "- вызывать `Health` с таймаутом;\n",
    "- вызывать `Predict` с набором признаков Iris.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ajR20tetHIVn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ client.py создан.\n",
      "import grpc\n",
      "import model_pb2, model_pb2_grpc  # Imports from root\n",
      "\n",
      "def make_stub(addr: str = \"localhost:50051\"):\n",
      "    channel = grpc.insecure_channel(addr)\n",
      "    return model_pb2_grpc.PredictionServiceStub(channel)\n",
      "\n",
      "def health(stub):\n",
      "    res = stub.Health(model_pb2.HealthRequest(), timeout=2.0)\n",
      "    print(\"Health:\", res.status, \"version:\", res.model_version)\n",
      "\n",
      "def predict(stub):\n",
      "    req = model_pb2.PredictRequest(features=[\n",
      "        model_pb2.Feature(name=\"sepal_length\", value=5.1),\n",
      "        model_pb2.Feature(name=\"sepal_width\", value=3.5),\n",
      "        model_pb2.Feature(name=\"petal_length\", value=1.4),\n",
      "        model_pb2.Feature(name=\"petal_width\", value=0.2),\n",
      "    ])\n",
      "    res = stub.Predict(req, timeout=3.0)\n",
      "    print(\"Prediction:\", res.prediction, \"confidence:\", round(res.confidence, 4), \"version:\", res.model_version)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    stub = make_stub()\n",
      "    health(stub)\n",
      "    predict(stub)"
     ]
    }
   ],
   "source": [
    "client_code = \"\"\"\n",
    "import grpc\n",
    "import model_pb2, model_pb2_grpc  # Imports from root\n",
    "\n",
    "def make_stub(addr: str = \"localhost:50051\"):\n",
    "    channel = grpc.insecure_channel(addr)\n",
    "    return model_pb2_grpc.PredictionServiceStub(channel)\n",
    "\n",
    "def health(stub):\n",
    "    res = stub.Health(model_pb2.HealthRequest(), timeout=2.0)\n",
    "    print(\"Health:\", res.status, \"version:\", res.model_version)\n",
    "\n",
    "def predict(stub):\n",
    "    req = model_pb2.PredictRequest(features=[\n",
    "        model_pb2.Feature(name=\"sepal_length\", value=5.1),\n",
    "        model_pb2.Feature(name=\"sepal_width\", value=3.5),\n",
    "        model_pb2.Feature(name=\"petal_length\", value=1.4),\n",
    "        model_pb2.Feature(name=\"petal_width\", value=0.2),\n",
    "    ])\n",
    "    res = stub.Predict(req, timeout=3.0)\n",
    "    print(\"Prediction:\", res.prediction, \"confidence:\", round(res.confidence, 4), \"version:\", res.model_version)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stub = make_stub()\n",
    "    health(stub)\n",
    "    predict(stub)\n",
    "\"\"\"\n",
    "with open(\"client/client.py\", \"w\") as f:\n",
    "    f.write(client_code.strip())\n",
    "\n",
    "print(\"✅ client.py создан.\")\n",
    "!sed -n '1,120p' client/client.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfRvoARKHKJ_"
   },
   "source": [
    "## **8. Подготовка модели `model.pkl`**\n",
    "\n",
    "Для демонстрации обучим простую `LogisticRegression` на Iris и сохраним в `models/model.pkl`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iZrg8UWqHOWt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved with correct feature names\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd, joblib, os\n",
    "\n",
    "# Load Iris\n",
    "iris = datasets.load_iris(as_frame=True)\n",
    "df = iris.frame\n",
    "\n",
    "# Rename columns to match what client sends\n",
    "X = df[iris.feature_names].rename(columns={\n",
    "    'sepal length (cm)': 'sepal_length',\n",
    "    'sepal width (cm)': 'sepal_width', \n",
    "    'petal length (cm)': 'petal_length',\n",
    "    'petal width (cm)': 'petal_width'\n",
    "})\n",
    "y = df['target']\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Save\n",
    "os.makedirs(\"ml_grpc_service/models\", exist_ok=True)\n",
    "joblib.dump(model, \"ml_grpc_service/models/model.pkl\")\n",
    "print(\"✅ Model saved with correct feature names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIEFrEIfHPzB"
   },
   "source": [
    "## **9. Запуск сервера gRPC**\n",
    "\n",
    "Поднимем сервер в фоне и посмотрим его лог.  \n",
    "По умолчанию: `PORT=50051`, `MODEL_PATH=models/model.pkl`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qXpQWj29HRLr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "File \"<frozen runpy>\", line 88, in _run_code\n",
      "File \"/Users/katrindar/MIPT/Masters/ml_grpc_service/server/server.py\", line 3, in <module>\n"
     ]
    }
   ],
   "source": [
    "import os, subprocess, time, sys, signal\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"PYTHONPATH\"] = os.getcwd()  # чтобы пакеты model_pb2 корректно импортировались\n",
    "# при желании можно эмулировать задержку предсказания:\n",
    "# env[\"SLEEP_MS\"] = \"0\"\n",
    "\n",
    "server_proc = subprocess.Popen(\n",
    "    [sys.executable, \"-m\", \"server.server\"],\n",
    "    env=env,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "time.sleep(1.2)  # дать серверу стартовать\n",
    "\n",
    "# показать первые строки лога\n",
    "for _ in range(4):\n",
    "    line = server_proc.stdout.readline().strip()\n",
    "    if line:\n",
    "        print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMGhqwy2HTRA"
   },
   "source": [
    "## **10. Проверка Health и Predict**\n",
    "\n",
    "Вызовем методы через клиента: `/Health` и `/Predict`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "67yHyWENHUeW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': '<run_path>',\n",
       " '__doc__': None,\n",
       " '__package__': '',\n",
       " '__loader__': None,\n",
       " '__spec__': None,\n",
       " '__file__': 'client/client.py',\n",
       " '__cached__': None,\n",
       " '__builtins__': {'__name__': 'builtins',\n",
       "  '__doc__': \"Built-in functions, types, exceptions, and other objects.\\n\\nThis module provides direct access to all 'built-in'\\nidentifiers of Python; for example, builtins.len is\\nthe full name for the built-in function len().\\n\\nThis module is not normally accessed explicitly by most\\napplications, but can be useful in modules that provide\\nobjects with the same name as a built-in value, but in\\nwhich the built-in of that name is also needed.\",\n",
       "  '__package__': '',\n",
       "  '__loader__': _frozen_importlib.BuiltinImporter,\n",
       "  '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>, origin='built-in'),\n",
       "  '__build_class__': <function __build_class__>,\n",
       "  '__import__': <function __import__(name, globals=None, locals=None, fromlist=(), level=0)>,\n",
       "  'abs': <function abs(x, /)>,\n",
       "  'all': <function all(iterable, /)>,\n",
       "  'any': <function any(iterable, /)>,\n",
       "  'ascii': <function ascii(obj, /)>,\n",
       "  'bin': <function bin(number, /)>,\n",
       "  'breakpoint': <function breakpoint>,\n",
       "  'callable': <function callable(obj, /)>,\n",
       "  'chr': <function chr(i, /)>,\n",
       "  'compile': <function compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1, *, _feature_version=-1)>,\n",
       "  'delattr': <function delattr(obj, name, /)>,\n",
       "  'dir': <function dir>,\n",
       "  'divmod': <function divmod(x, y, /)>,\n",
       "  'eval': <function eval(source, globals=None, locals=None, /)>,\n",
       "  'exec': <function exec(source, globals=None, locals=None, /, *, closure=None)>,\n",
       "  'format': <function format(value, format_spec='', /)>,\n",
       "  'getattr': <function getattr>,\n",
       "  'globals': <function globals()>,\n",
       "  'hasattr': <function hasattr(obj, name, /)>,\n",
       "  'hash': <function hash(obj, /)>,\n",
       "  'hex': <function hex(number, /)>,\n",
       "  'id': <function id(obj, /)>,\n",
       "  'input': <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x107388410>>,\n",
       "  'isinstance': <function isinstance(obj, class_or_tuple, /)>,\n",
       "  'issubclass': <function issubclass(cls, class_or_tuple, /)>,\n",
       "  'iter': <function iter>,\n",
       "  'aiter': <function aiter(async_iterable, /)>,\n",
       "  'len': <function len(obj, /)>,\n",
       "  'locals': <function locals()>,\n",
       "  'max': <function max>,\n",
       "  'min': <function min>,\n",
       "  'next': <function next>,\n",
       "  'anext': <function anext>,\n",
       "  'oct': <function oct(number, /)>,\n",
       "  'ord': <function ord(c, /)>,\n",
       "  'pow': <function pow(base, exp, mod=None)>,\n",
       "  'print': <function print(*args, sep=' ', end='\\n', file=None, flush=False)>,\n",
       "  'repr': <function repr(obj, /)>,\n",
       "  'round': <function round(number, ndigits=None)>,\n",
       "  'setattr': <function setattr(obj, name, value, /)>,\n",
       "  'sorted': <function sorted(iterable, /, *, key=None, reverse=False)>,\n",
       "  'sum': <function sum(iterable, /, start=0)>,\n",
       "  'vars': <function vars>,\n",
       "  'None': None,\n",
       "  'Ellipsis': Ellipsis,\n",
       "  'NotImplemented': NotImplemented,\n",
       "  'False': False,\n",
       "  'True': True,\n",
       "  'bool': bool,\n",
       "  'memoryview': memoryview,\n",
       "  'bytearray': bytearray,\n",
       "  'bytes': bytes,\n",
       "  'classmethod': classmethod,\n",
       "  'complex': complex,\n",
       "  'dict': dict,\n",
       "  'enumerate': enumerate,\n",
       "  'filter': filter,\n",
       "  'float': float,\n",
       "  'frozenset': frozenset,\n",
       "  'property': property,\n",
       "  'int': int,\n",
       "  'list': list,\n",
       "  'map': map,\n",
       "  'object': object,\n",
       "  'range': range,\n",
       "  'reversed': reversed,\n",
       "  'set': set,\n",
       "  'slice': slice,\n",
       "  'staticmethod': staticmethod,\n",
       "  'str': str,\n",
       "  'super': super,\n",
       "  'tuple': tuple,\n",
       "  'type': type,\n",
       "  'zip': zip,\n",
       "  '__debug__': True,\n",
       "  'BaseException': BaseException,\n",
       "  'BaseExceptionGroup': BaseExceptionGroup,\n",
       "  'Exception': Exception,\n",
       "  'GeneratorExit': GeneratorExit,\n",
       "  'KeyboardInterrupt': KeyboardInterrupt,\n",
       "  'SystemExit': SystemExit,\n",
       "  'ArithmeticError': ArithmeticError,\n",
       "  'AssertionError': AssertionError,\n",
       "  'AttributeError': AttributeError,\n",
       "  'BufferError': BufferError,\n",
       "  'EOFError': EOFError,\n",
       "  'ImportError': ImportError,\n",
       "  'LookupError': LookupError,\n",
       "  'MemoryError': MemoryError,\n",
       "  'NameError': NameError,\n",
       "  'OSError': OSError,\n",
       "  'ReferenceError': ReferenceError,\n",
       "  'RuntimeError': RuntimeError,\n",
       "  'StopAsyncIteration': StopAsyncIteration,\n",
       "  'StopIteration': StopIteration,\n",
       "  'SyntaxError': SyntaxError,\n",
       "  'SystemError': SystemError,\n",
       "  'TypeError': TypeError,\n",
       "  'ValueError': ValueError,\n",
       "  'Warning': Warning,\n",
       "  'FloatingPointError': FloatingPointError,\n",
       "  'OverflowError': OverflowError,\n",
       "  'ZeroDivisionError': ZeroDivisionError,\n",
       "  'BytesWarning': BytesWarning,\n",
       "  'DeprecationWarning': DeprecationWarning,\n",
       "  'EncodingWarning': EncodingWarning,\n",
       "  'FutureWarning': FutureWarning,\n",
       "  'ImportWarning': ImportWarning,\n",
       "  'PendingDeprecationWarning': PendingDeprecationWarning,\n",
       "  'ResourceWarning': ResourceWarning,\n",
       "  'RuntimeWarning': RuntimeWarning,\n",
       "  'SyntaxWarning': SyntaxWarning,\n",
       "  'UnicodeWarning': UnicodeWarning,\n",
       "  'UserWarning': UserWarning,\n",
       "  'BlockingIOError': BlockingIOError,\n",
       "  'ChildProcessError': ChildProcessError,\n",
       "  'ConnectionError': ConnectionError,\n",
       "  'FileExistsError': FileExistsError,\n",
       "  'FileNotFoundError': FileNotFoundError,\n",
       "  'InterruptedError': InterruptedError,\n",
       "  'IsADirectoryError': IsADirectoryError,\n",
       "  'NotADirectoryError': NotADirectoryError,\n",
       "  'PermissionError': PermissionError,\n",
       "  'ProcessLookupError': ProcessLookupError,\n",
       "  'TimeoutError': TimeoutError,\n",
       "  'IndentationError': IndentationError,\n",
       "  'IndexError': IndexError,\n",
       "  'KeyError': KeyError,\n",
       "  'ModuleNotFoundError': ModuleNotFoundError,\n",
       "  'NotImplementedError': NotImplementedError,\n",
       "  'RecursionError': RecursionError,\n",
       "  'UnboundLocalError': UnboundLocalError,\n",
       "  'UnicodeError': UnicodeError,\n",
       "  'BrokenPipeError': BrokenPipeError,\n",
       "  'ConnectionAbortedError': ConnectionAbortedError,\n",
       "  'ConnectionRefusedError': ConnectionRefusedError,\n",
       "  'ConnectionResetError': ConnectionResetError,\n",
       "  'TabError': TabError,\n",
       "  'UnicodeDecodeError': UnicodeDecodeError,\n",
       "  'UnicodeEncodeError': UnicodeEncodeError,\n",
       "  'UnicodeTranslateError': UnicodeTranslateError,\n",
       "  'ExceptionGroup': ExceptionGroup,\n",
       "  'EnvironmentError': OSError,\n",
       "  'IOError': OSError,\n",
       "  'open': <function _io.open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)>,\n",
       "  'copyright': Copyright (c) 2001-2023 Python Software Foundation.\n",
       "  All Rights Reserved.\n",
       "  \n",
       "  Copyright (c) 2000 BeOpen.com.\n",
       "  All Rights Reserved.\n",
       "  \n",
       "  Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
       "  All Rights Reserved.\n",
       "  \n",
       "  Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
       "  All Rights Reserved.,\n",
       "  'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n",
       "      for supporting Python development.  See www.python.org for more information.,\n",
       "  'license': Type license() to see the full license text,\n",
       "  'help': Type help() for interactive help, or help(object) for help about object.,\n",
       "  'execfile': <function _pydev_bundle._pydev_execfile.execfile(file, glob=None, loc=None)>,\n",
       "  'runfile': <function _pydev_bundle.pydev_umd.runfile(filename, args=None, wdir=None, namespace=None)>,\n",
       "  '__IPYTHON__': True,\n",
       "  'display': <function IPython.core.display_functions.display(*objs, include=None, exclude=None, metadata=None, transient=None, display_id=None, raw=False, clear=False, **kwargs)>,\n",
       "  'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x107a56a80>>},\n",
       " 'grpc': <module 'grpc' from '/Users/katrindar/MIPT/Masters/ml_grpc_service/.venv/lib/python3.12/site-packages/grpc/__init__.py'>,\n",
       " 'model_pb2': <module 'protos.model_pb2' from '/Users/katrindar/MIPT/Masters/ml_grpc_service/protos/model_pb2.py'>,\n",
       " 'model_pb2_grpc': <module 'protos.model_pb2_grpc' from '/Users/katrindar/MIPT/Masters/ml_grpc_service/protos/model_pb2_grpc.py'>,\n",
       " 'make_stub': <function <run_path>.make_stub(addr: str = 'localhost:50051')>,\n",
       " 'health': <function <run_path>.health(stub)>,\n",
       " 'predict': <function <run_path>.predict(stub)>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import runpy, os, sys\n",
    "\n",
    "# Запуск клиента как модуля\n",
    "os.environ[\"PYTHONPATH\"] = os.getcwd()\n",
    "runpy.run_path(\"client/client.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_og9AXDHXi6"
   },
   "source": [
    "## **11. Негативные сценарии: валидация и таймауты**\n",
    "\n",
    "1) Попробуем отправить **дубликат признака** — сервер должен вернуть `INVALID_ARGUMENT`.  \n",
    "2) Эмулируем **таймаут** клиента: включим задержку на сервере через переменную окружения `SLEEP_MS`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "sg3rLd0XHX0D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate feature -> status: INVALID_ARGUMENT | details: Duplicate feature: sepal_length\n",
      "Timeout test -> status: UNAVAILABLE | details: Socket closed\n"
     ]
    }
   ],
   "source": [
    "# 11.1 Дубликат признака -> INVALID_ARGUMENT\n",
    "import grpc\n",
    "from protos import model_pb2, model_pb2_grpc\n",
    "\n",
    "stub = model_pb2_grpc.PredictionServiceStub(grpc.insecure_channel(\"localhost:50051\"))\n",
    "\n",
    "bad_req = model_pb2.PredictRequest(features=[\n",
    "    model_pb2.Feature(name=\"sepal_length\", value=5.1),\n",
    "    model_pb2.Feature(name=\"sepal_length\", value=5.2),  # дубликат\n",
    "])\n",
    "try:\n",
    "    _ = stub.Predict(bad_req, timeout=3.0)\n",
    "except grpc.RpcError as e:\n",
    "    print(\"Duplicate feature -> status:\", e.code().name, \"| details:\", e.details())\n",
    "\n",
    "# 11.2 Таймаут клиента: перезапустим сервер с задержкой\n",
    "import os, subprocess, sys, time, signal\n",
    "\n",
    "# остановим текущий сервер\n",
    "server_proc.send_signal(signal.SIGINT)\n",
    "server_proc.wait(timeout=3)\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"PYTHONPATH\"] = os.getcwd()\n",
    "env[\"SLEEP_MS\"] = \"500\"  # 0.5 секунды задержка на предсказание\n",
    "\n",
    "server_proc = subprocess.Popen(\n",
    "    [sys.executable, \"-m\", \"server.server\"],\n",
    "    env=env,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True\n",
    ")\n",
    "time.sleep(1.2)\n",
    "\n",
    "# попробуем вызвать с слишком маленьким timeout\n",
    "try:\n",
    "    _ = stub.Predict(\n",
    "        model_pb2.PredictRequest(features=[\n",
    "            model_pb2.Feature(name=\"sepal_length\", value=5.1),\n",
    "            model_pb2.Feature(name=\"sepal_width\", value=3.5),\n",
    "            model_pb2.Feature(name=\"petal_length\", value=1.4),\n",
    "            model_pb2.Feature(name=\"petal_width\", value=0.2),\n",
    "        ]),\n",
    "        timeout=0.2,  # 200 мс < задержки 500 мс\n",
    "    )\n",
    "except grpc.RpcError as e:\n",
    "    print(\"Timeout test -> status:\", e.code().name, \"| details:\", e.details())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQocRT_uHZO9"
   },
   "source": [
    "## **12. Мини-бенчмарк: 100 последовательных предсказаний**\n",
    "\n",
    "Сделаем простой прогон 100 RPC-вызовов `Predict` и измерим суммарное время.  \n",
    "(Для корректных результатов установите `SLEEP_MS=0` или не задавайте переменную.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "oiUeTTvlHbO0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Predict RPCs in 0.080 s, avg 0.80 ms/call\n"
     ]
    }
   ],
   "source": [
    "# перезапустим сервер без задержки\n",
    "import signal, time, os, sys, subprocess\n",
    "\n",
    "server_proc.send_signal(signal.SIGINT)\n",
    "server_proc.wait(timeout=3)\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"PYTHONPATH\"] = os.getcwd()\n",
    "env.pop(\"SLEEP_MS\", None)\n",
    "\n",
    "server_proc = subprocess.Popen(\n",
    "    [sys.executable, \"-m\", \"server.server\"],\n",
    "    env=env,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True\n",
    ")\n",
    "time.sleep(1.2)\n",
    "\n",
    "# бенчмарк\n",
    "import grpc, time\n",
    "from protos import model_pb2, model_pb2_grpc\n",
    "\n",
    "stub = model_pb2_grpc.PredictionServiceStub(grpc.insecure_channel(\"localhost:50051\"))\n",
    "\n",
    "req = model_pb2.PredictRequest(features=[\n",
    "    model_pb2.Feature(name=\"sepal_length\", value=5.1),\n",
    "    model_pb2.Feature(name=\"sepal_width\", value=3.5),\n",
    "    model_pb2.Feature(name=\"petal_length\", value=1.4),\n",
    "    model_pb2.Feature(name=\"petal_width\", value=0.2),\n",
    "])\n",
    "\n",
    "t0 = time.time()\n",
    "N = 100\n",
    "for _ in range(N):\n",
    "    _ = stub.Predict(req, timeout=2.0)\n",
    "t1 = time.time()\n",
    "\n",
    "print(f\"{N} Predict RPCs in {t1 - t0:.3f} s, avg {(t1 - t0)/N*1000:.2f} ms/call\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMoWi2JiHckx"
   },
   "source": [
    "## **13. Обновление сервера для опциональной задержки (для тестов таймаута)**\n",
    "\n",
    "Если вы хотите оставить поддержку переменной `SLEEP_MS`, обновите `server.py`,  \n",
    "добавив внутри `Predict` небольшой `sleep`. Это удобно для учебных тестов таймаутов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "oBrWesbHHd9_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ server.py обновлён (SLEEP_MS поддерживается).\n",
      "import grpc, os\n",
      "from concurrent import futures\n",
      "from protos import model_pb2, model_pb2_grpc  # Imports from root\n",
      "from server.inference import ModelRunner\n",
      "from server.validation import features_to_dict, ValidationError\n",
      "\n",
      "MODEL_PATH = os.getenv(\"MODEL_PATH\", \"models/model.pkl\")\n",
      "MODEL_VERSION = os.getenv(\"MODEL_VERSION\", \"v1.0.0\")\n",
      "MAX_WORKERS = int(os.getenv(\"MAX_WORKERS\", \"4\"))\n",
      "PORT = int(os.getenv(\"PORT\", \"50051\"))\n",
      "\n",
      "class PredictionService(model_pb2_grpc.PredictionServiceServicer):\n",
      "    def __init__(self):\n",
      "        self.runner = ModelRunner(MODEL_PATH, version=MODEL_VERSION)\n",
      "\n",
      "    def Health(self, request, context):\n",
      "        return model_pb2.HealthResponse(status=\"ok\", model_version=self.runner.version)\n",
      "\n",
      "    def Predict(self, request, context):\n",
      "        import os, time\n",
      "        sleep_ms = int(os.getenv('SLEEP_MS', '0'))\n",
      "        if sleep_ms > 0:\n",
      "            time.sleep(sleep_ms / 1000.0)\n",
      "        try:\n",
      "            feats = features_to_dict(request.features)\n",
      "            pred, conf = self.runner.predict(feats)\n",
      "            return model_pb2.PredictResponse(\n",
      "                prediction=pred, confidence=conf, model_version=self.runner.version\n",
      "            )\n",
      "        except ValidationError as ve:\n",
      "            context.set_code(grpc.StatusCode.INVALID_ARGUMENT)\n",
      "            context.set_details(str(ve))\n",
      "            return model_pb2.PredictResponse()\n",
      "        except Exception as e:\n",
      "            context.set_code(grpc.StatusCode.INTERNAL)\n",
      "            context.set_details(f\"internal error: {e}\")\n",
      "            return model_pb2.PredictResponse()\n",
      "\n",
      "def serve():\n",
      "    options = [\n",
      "        (\"grpc.max_send_message_length\", 50 * 1024 * 1024),\n",
      "        (\"grpc.max_receive_message_length\", 50 * 1024 * 1024),\n",
      "    ]\n",
      "    server = grpc.server(futures.ThreadPoolExecutor(max_workers=MAX_WORKERS), options=options)\n",
      "    model_pb2_grpc.add_PredictionServiceServicer_to_server(PredictionService(), server)\n",
      "    server.add_insecure_port(f\"[::]:{PORT}\")\n",
      "    server.start()\n",
      "    print(f\"gRPC server started on :{PORT}, model={MODEL_PATH}, version={MODEL_VERSION}\")\n",
      "    server.wait_for_termination()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    try:\n",
      "        import uvloop\n",
      "        uvloop.install()\n",
      "    except Exception:\n",
      "        pass\n",
      "    serve()"
     ]
    }
   ],
   "source": [
    "# Патчим server.py: добавим поддержку задержки через env SLEEP_MS\n",
    "from pathlib import Path\n",
    "\n",
    "p = Path(\"server/server.py\")\n",
    "code = p.read_text()\n",
    "\n",
    "if \"SLEEP_MS\" not in code:\n",
    "    code = code.replace(\n",
    "        \"def Predict(self, request, context):\",\n",
    "        \"def Predict(self, request, context):\\n        import os, time\\n        sleep_ms = int(os.getenv('SLEEP_MS', '0'))\\n        if sleep_ms > 0:\\n            time.sleep(sleep_ms / 1000.0)\"\n",
    "    )\n",
    "    p.write_text(code)\n",
    "\n",
    "print(\"✅ server.py обновлён (SLEEP_MS поддерживается).\")\n",
    "!sed -n '1,140p' server/server.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piDLJBgVHfjO"
   },
   "source": [
    "## **14. Типовые ошибки и отладка**\n",
    "\n",
    "- **Импорты `model_pb2` не находятся.** Проверьте, куда `protoc` вывел файлы и корректно ли выставлен `PYTHONPATH`.  \n",
    "- **Сервер не видит модель.** Убедитесь, что `MODEL_PATH` указывает на реальный `model.pkl` и текущая рабочая директория верна.  \n",
    "- **Клиент «висит».** Всегда задавайте `timeout` в RPC-вызовах.  \n",
    "- **Изменили `.proto`, а код не обновили.** Регенерируйте Python-модули `model_pb2*.py` на сервере и клиенте.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6rh6Rd3HlSx"
   },
   "source": [
    "## **15. Чек-лист готовности к контейнеризации**\n",
    "\n",
    "- Контракт в `.proto` описан и хранится в VCS.  \n",
    "- Python-код сгенерирован автоматикой (`grpcio-tools`) и не редактируется вручную.  \n",
    "- Сервер поднимается, `/Health` возвращает `ok` и `model_version`.  \n",
    "- `/Predict` работает, ошибки валидируются (дубликаты, пустые признаки).  \n",
    "- Клиент использует таймауты и реализует сценарии «счастливый путь» и негативные кейсы.  \n",
    "- Проект структурирован; зависимости закреплены в `requirements.txt`.  \n",
    "- Параметры (`PORT`, `MODEL_PATH`, `MODEL_VERSION`, опционально `SLEEP_MS`) вынесены в переменные окружения.  \n",
    "- Готово к контейнеризации (Dockerfile + `ENTRYPOINT` → следующий семинар)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOpVt7fst1H8mRe20SDh9yN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
